#summary Evaluation server for object recognition.
#labels Evaluation,Intro

= Introduction =

The server allows to upload submissions, it keeps track of the submitted results and calls custom processing engine to do checking/evaluation.

The evaluation is done in 2 steps:
 1. on upload, the content is verified and an error report is generated if the content is bad. 
 1. An independent process visits all valid submissions and runs the evaluation. The result (score or error) is written to the submission.

= Components =

  * Django server. Does user registration, uploads the submissions, keep the status of the submissions and their scores.
  * Submission check function django/web_annotations_server/evaluation/eval_VOCcheck.py
  * Standalone checking function django/web_annotations_server/evaluation/standalone_evaluation.py
  * Matlab wrappers for VOC dev kit to run evaluation on submissions.

= eval_VOCcheck.py =

eval_VOCcheck.py unpacks and checks the submission. It has the following parameters:

|| Parameter || Meaning ||
|| --submission=<path_to_file>   || Full filename of the submission ||
|| --work_root=<path_to_folder>  || A place, where to extract the submission ||
|| --report=<path_to_file>       || report filename. ||
|| --devkit=<path_to_VOCdevkit>  || A path to VOC dev kit. We need image sets   ||
|| --challenge=<challenge> || Challenge year (VOC2008/VOC2009). Used to get the right image set ||
|| --setname=<eval_set_name> || The name of the evaluation set. E.g. *val* or *test* || 


= Django server notes =
 
 * Evaluation app needs:
   * [code.google.com/p/django-registration/ django-registration]  
   * settings.VOC_DEV_KIT='......../VOCdevkit'
   * settings.MCR_ROOT='......./MATLAB/MATLAB_Compiler_Runtime/v79'  (example)
 
